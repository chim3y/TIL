## Saver class
* '가중치의 상태'와 체크포인트(구성된 모든 그래프 모두에 대한 정보를 포함한 직렬화된 모델 관리
1. tf.train.Saver() 로 saver instance 생성
(이 때 유지할 최근 변수 체크포인트 수를 지정해야하고, 선택적으로 체크포인트 유지 시간 간격을 지정할 수 있다)
``` python
saver=tf.train.Saver(max_to_keep=7, keep_checkpoint_every_n_hours=0.5)
# 최근 7개의 체크포인트만 유지하고 부가적으로 30분마다 하나의 체크포인트를 유지하도록 지정한 것
```
2. .save() 로 체크포인트 저장
세션 인수, 파일 저장할 경로, 스텝수(global step, 반복 회수 나타내는 지표를 각 체크포인트 파일명과 자동으로 연결됨)을 인수로 받는다. 
``` python
DIR="path/to/model"

with tf.Session() as sess:
	for step in range(1, NUM_STEPS+1):
		batch_xs, batch_ys=data.train.next_batch(MINIBATCH_SIZE)
		sess.run(gd_step, feed_dict=x: batch_xs, y_true: batch_ys)

		if step % 50 ==0:
			saver.save(sess, os.path.join(DIR, "model"),
						global_step=step)
```

3. saver.restore()로 동일한 그래프 모델에 원하는 체크포인트를 복원
``` python
tf.reset_default_graph()
x=tf.placeholder(tf.float32, [None, 784], name='X')
W=tf.Variable(tf.zeros([784, 10]), name='W')
y_true=tf.placeholder(tf.float32, [None, 10])
y_pred=tf.matmul(x, W) cross_entropy=tf.reduce_mean(
			tf.nn.softmax_cross_entropy_with_logits(logits=y_pred,
													labels=y_true))
gd_step=tf.train.GradientDescentOptimizer(0.5)\
				.minimize(cross_entropy)
correct_mask=tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))
accuracy=tf.reduce_mean(tf.cast(correct_mask, tf.float32))

saver=tf.train.Saver()

with tf.Session() as sess:
	saver.restore(sess, os.path.join(DIR, "model_ckpt-1000"))
	ans=sess.run(accuracy, feed_dict={x: data.test.images, 
									  y_true: data.test.labels})
```
* MetaGraphDef : 그래프 및 그래프에 저장된 가중치 통합하는 방법에 대한 정보(메타정보)
	* 프로토콜 버퍼를 활용해서 직렬화(스트링으로 변환)되며, 구성하는 정보에 따라 여러 부분으로 나뉨
	* (네트워크 구조 정보는 graph_def 에 저장)
* 저장된 그래프 불러오기
	* tf.train.import_meta_graph() 호출 --> .meta 확장자를 가진 복원하고자 하는 체크포인트 파일명을 인수로 호출함
``` python
tf.reset_default_graph()
DIR="path/to/model"
with tf.Session() as sess:
	saver=tf.train.import_meta_graph(os.path.join(DIR, "model_ckpt-1000.meta"))
	saver.restore(sess, os.path.join(DIR, "model_ckpt-1000"))
	ans=sess.run(accuracy, feed_dict={x: data.test.images,
									  y_true: data.test.labels})

```
* 컬렉션
	* 모델을 가져와서 가중치를 복원해도 세션을 실행할 때 인수로 사용하는 변수에 대한 추가접근을 제공하지 않음(fetches & feed_dict의 키) --> 컬렉션으로 해결 가능
	* 컬렉션 : 덱셔너리와 비슷한 텐서플로 객체
	* 그래프 구성요소를 순서에 따라 접근 가능한 방식으로 보관할 수 있음
``` python
# fetch 하려는 변수(accuracy) 와 밀어넣을 데이터의 키(x, y_true)를 컬렉션에 넣기
train_var=[x, y_true, accuracy]
tf.add_to_collection('train_var', train_var[0])
tf.add_to_collection('train_var', train_var[1])
tf.add_to_collection('train_var', train_var[2])
# 가중치 체크포인트, 그래프 구조를 자동으로 저장
saver=tf.train.Saver(max_to_keep=7,
					 keep_checkpoint_every_n_hours=1)
# 명시적으로 그래프 저장하고, 컬렉션 추가(두번째 인수)
saver.export_meta_graph(os.path.join(DIR, "model_ckpt.meta"),
						collection_list=['train_var'])
```
``` python
tf.reset_default_graph()
DIR="path/to/model"

with tf.Session() as sess:
	sess.run(tf.global_variables_initializer())
	saver=tf.train.import_meta_graph(os.path.join(
									 DIR, "model_ckpt.meta"))
	saver.restore(sess, os.path.join(DIR, "model_ckpt-1000"))
	x=tf.get_collection('train_var')[0]
	y_true=tf.get_collection('train_var')[1]
	accuracy=tf.get_ollection('train_var')[2]
	ans=sess.run(accuracy, feed_dict={x: data.test.images,
									  y_true: data.test.labels})
```


### 매개변수 재할당
* 뭘로 하던 매개변수 재할당하려면 그래프를 재작성해야하지만, saver 사용해서 필요한 모든 정보가 포함된 .meta 체크포인트 파일 생성하면 그래프 재구성 없이 그래프 복원 가능


## 복원전 그래프 재설정
* 읽어들인 변수는 현재의 그래프의 변수와 쌍이 맞아야해서 같은 이름을 가져야 함.
* 어떤 이유로든 이름이 일치하지 않으면 에러 발생
* 아래 코드로 그래프 재설종해줘야 해결
``` python
tf.reset_default_graph()
```

## tensorflow 에서의 model 에서
* tensorflow model 



참고 : https://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/
http://jaynewho.com/post/8