## 9. Data Source
* 스파크 핵심 데이터 소스를 이용해 데이터 읽고 쓰는 방법 터득
* 서드 파티 데이터소스와 스파크 연동시 고려해야할 점 공부

### 9.1 데이터소스 api 구조
#### 9.1.1 읽기 api 구조
* 모든 데이터소스를 읽을때 아래와 같은 형식 사용
```scala
DataFrameReader.format(...).option("key", "value").schema(...).load()
```
* format - optional & 기본값은 파케이
* option - 데이터 읽는 방법을 설정
* schema - 데이터 소스에서 스키마를 제공하거나, 스키마 추론기능을 사용하려는 경우에 선택적으로 사용

#### 9.1.2 데이터 읽기의 기초
* 기본적으로 DataFrameReader 사용
* DataFrameReader 는 SparkSession의 read 속성으로 접근
* 포맷, 스키마, 읽기모드, 옵션 지정
* 포맷, 스키마, 옵션은 transformation 을 추가로 정의할 수 있는 DataFrameReader 반환 / 필요한경우에만 선택적으로 지정할 수 있음
* 읽을 경로는 반드시 지정해야 함
```scala
spark.read.format("csv")
	.option("mode", "FAILFAST")
	.option("inferSchema", "true")
	.option("path", "path/to/file(s)")
	.schema(someSchema)
	.load()
```
* 읽기 모드
	* 형식에 맞지 않는 데이터를 만났을 때의 동작방식 지정
		- permissive(기본값) : 오류 레코드의 모든 필드를 null 로 설정하고 모든 오류 레코드를 \_corrupt_record 라는 문자열 컬럼에 기록함
		- dropMalformed :  형식에 맞지 않는 row 제거
		- failFast : 형식에 맞지 않는 레코드 만나면 즉시 종료

#### 9.1.3. 쓰기 api 구조
- format  - 기본값은 파케이
```scala
DataFrameWriter.format([선택가능 - 기본은 파케이]).option(...).partitionBy(...).bucketBy(...).sortBy(...).save()
```
- partitionBy, bucketBy, sortBy
	- 최종 파일 배치 형태 제어
	- 파일기반 데이터소스에서만 동작

#### 9.1.4 데이터 쓰기 기초
- DataFrame 의 write 속성 이용, DataFrame 별로 DataFrameWriter 에 접근해야 함
```scala
dataframe.write.format("csv")
	.option("mode", "OVERWRITE")
	.option("dataFormat", "yyyy-MM-dd")
	.option("path", "path/to/file(s)")
	.save()
```
- mode : append/overwrite/errorIfExists(기본값)/ignore(해당 경로에 뭔가 있으면 아무거도 하지 않는다)

### 9.2 csv
#### 9.2.1 csv option
#### 9.2.2 csv 파일 읽기
1. csv 용 DataFrameReader 생성
```scala
spark.read.format("csv")
```
2. option 지정
```scala
spark.read.format("csv")
	.option("header", "true")
	.option("mode", "FAILFAST")
	.option("inferSchema", "true")
	.load("some/path/to/file.csv")
```
- 스키마 파일의 데이터가 예상한 형태로 돼있는지를 검증하는 용도로 사용할 수도 있다
```scala
import org.apache.spark.sql.types.{StructureField, StructureType, StringType, LongType}
val myManualSchema=new StructType(Array(
	new StructField("DEST_COUNTRY_NAME", StringType, true),
	new StructField("ORIGIN_COUNTRY_NAME", StringType, true),
	new StructField("count", LongType, false)
))
spark.read.format("csv")
	.option("header", "true")
	.option("mode", "FAILFAST")
	.schema(myManualSchema)
	.load("/data/flight-data/csv/2010-summary.csv")
	.show(5)
```

#### 9.2.3 csv 파일 쓰기
```scala
val csvFile=spark.read.format("csv")
	.option("header", "true").option("mode", "FAILFAST").schema(myManualSchema)
	.load("/data/flight-data/csv/2010-summary.csv")

//csv 로 읽어서 tsv 로 내보내기
csvFile.write.format("csv").mode("overwrite").option("sep", "\t").save("/tmp/my-tsv-file.tsv")
```

## 9.3 JSON 파일
- 스파크에서는 json 파일 사용시 '줄로 구분된 json'을 기본으로 사용
	- multiline 옵션 사용해서 선택적으로 사용해야 전체 파일을 하나의 json 으로 읽음
- json parsing 후 dataframe 생성
- 줄로 된 json 은 전체 파일을 읽은 후 저장하는 방식이 아니라서 새로운 레코드를 추가할 수 있음 - 다른 포맷에 비해 훨씬 안정적

### 9.3.1 json 옵션
### 9.3.2 json 파일 읽기
```scala
spark.read.format("json").option("mode", "FAILFAST").schema(myManualSchema)
	.load("/data/flight-data/json/2010-summary.json").show(5)
```
### 9.3.3 json 파일 쓰기
- 데이터 소스에 관계 없이 json 파일에 저장 가능
- 이때 파티션당 하나의 파일을 만들며 전체 dataframe 을 단일 폴더에 저장
- json 객체는 한줄에 하나씩 기록된다.
```scala
csvFile.write.format("json").mode("overwrite").save("/tmp/my-json-file.json")
```

## 9.4 파케이(parquet) 파일
- 컬럼기반 데이터 저장 방식

- 스파크의 기본 파일 포맷
- 읽기 연산에서 json. 이나 csv 보다 훨씬 효율적으로 동작하므로 장기 저장용은 파케이 방식이 적절함.
- 장점
	- 분석 워크로드에 최적화됨
		- 저장소 공간을 절약할 수 있음
		- 전체 파일을 읽는 대신 개별 컬럼을 읽을 수 있음
		- 컬럼 기반 압축 기능 제공
	- 복합데이터 타입 지원
		- 컬럼이 배열, 맵, 구조체 데이터 타입이라고 해도 문제없이 읽고 쓸 수 있음
		- (단 csv 에서는 배열 사용 불가)
- 주의
	- 호환되지 않는 파케이 파일을 다룰때는 문제가 생길 수 있음(다른 버전의 스파크를 사용하면 파케이 파일로 저장시 유의해야함)

### 9.4.1 파케이 파일 읽기
- 데이터 저장할때 자체 스키마를 사용해 데이터를 저장하기때문에 옵션은 거의 없음 (포맷 설정만으로 충분함)
- 읽는 시점에 스키마를 알 수 있음(schema on read) 파케이 파일은 스키마가 파일 자체에 내장되어있어서 추정이 필요하지 않음
```scala
spark.read.format("parquet")
	.load("/data/flight-data/parquet/2010-summary.parquet").show(5)
```

### 9.4.2 파케이 파일 쓰기
```scala
csvFile.write.format("parquet").mode("overwrite")
	.save("/tmp/my-parquet-file.parquet")
```





