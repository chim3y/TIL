	
* (MC로 하는 이유 —> 모든 상황에 대한 정보가 항상 있는건 이니니까 / env 와의 상호작용으로 얻을 수 있는 정보들만으로 구성해야 하니깐
* Next step in Dynamic programming setting
	* = getting action function 
		* convert V_pie to Q_pie
		* 근데 이걸 쓸 수가 없지
	* action value 구할 때에도 mc method ㅣ용
	* 단 이때에는 state와 action 을 동시해 고려해서 eward의 평균을 내야겠지
* value function 과 마찬가지로 여기서도 first visit mc method, every visit mc method 가 있겠지
	* 근데 episode 가 많아지면 수렴해서 뭘 쓰던 상관이 없지
* mc를 이용할 경우 deterministic policy 를 사용하게 되면 state X 에 대해 정해진 policy 로만 움직이기 때문에 아무리 많은 episode ㅡㄹ 반복해도 이런 경우에 대한 값은 구할 수가 없어
	* stochastic policy 를 써야지