## Sampling from a data stream
### Sampling a fixed proportion
* Naive Solution
	* (user, query, time)의 stream data일때
	* hash function 로 각 query 마다 random integer 부여하고, 특정 수 아니면 다 버린다
	- stream 양이 많아지면 sample 의 양도 많아진다는 단점이 있음
	- 중복 검색어 비율 등을 구할 때에 잘못된 값을 낼 수 있음
	* 중복 검색어의 비율을 구하고싶다면?
		- user 가 평균적으로, x 개 query 를 한번, d개 query 를 2번 던진다면, 중복 검색어 비율의 참값은 d/(x+d)
		- 근데 naive 하게, 10%의 query 만 남겨뒀다면, d의 중복값이 다 들어갈 비율은 d/100(=(1/10)\*(1/10)\*d) 일 것
		- duplicate 된 애들 중에서, 18d/100(=((1/10)\*(9/10)+(1/10)\*(9/10))\*d)개는, 1번만 count 됐을 것
		- 즉, fixed proportion 으로 추출했다면, 중복 검색어의 비율은 (d/(10x+19d)) (=(d/100)/((x/10)+(d/100)+(18d/100))) 로 잘못나오게 될 것
* **Generalized solution**
	* query 가 아닌 user 를 sample 하는 방식
	* (user, query, time)에서 key 를 user 로 둔다
	* key 를 k 개 bucket 에 uniformly hashing
	* 전체 query 의 a/b 비율을 꼽는다면, b 개의 bucket 중, a 개를 sample 하는 것


### Sampling a fixed-size sample
* overview
	* 메인 메모리에 올리는게 가능한정도만 남기는 형태일 것
	* fixed size 로 sampling 하면서도, 모든 데이터가 같은 확률로 뽑힐 수 있게 할 수 있을까
		- size s 을 sampling 한다면 s가 5일때와 7일때 특정 수가 등장하는 비율이 다르게 나올거야 --> 이런 문제를 해결하려면?
	* 어떻게 하면 항상 적절하게 sampling 할 수 있을지를 고민하는 것 (전체 중에 s 개를 뽑았을때의 확률과 임의로 s개 뽑았을 떄의 확률을 같게 만드는게 목표인 것 --> sampling 을 잘 하고싶다!)
* **Reservoir Sampling**
	* 들어온 순서대로 s 개 element 저장할수 있다고 치면, 
	* n-1(n>s)개의 element 를 봤다고 할때, n 번째 element 가 도착한다면
	* --> s/n 의 확률로, n 을 pick 하고 아니면 버린다
	* 이 n 번째 element 가 pick 된다면 s개의 element 중 하나를 replace 한다.
* Reservoir Sampling 증명(inductive step)
	0. base
		- n=s 이라면 이미 desired property 만족함 --> 
	1. n번째 element 까지 했을때에, sample S 가 포함한 각 element 의 확률은 s/n
	2. n+1번째 element 가 왔을 때에
		1. n+1번째가 discard 되어 이전의 sampling 이 유지
			(1-s/(n+1))
		or
		2. n+1번째가 채택되어 뽑혀있던 s 개 중 1개가 out
			(s/(n+1))\*((s-1)/s)
		--> 1+2 = n/(n+1)
	--> 전체 element 가 n+1개일 때에 특정 tuple 이 s 에 등장할 확률 = (s/n) * (n/(n+1)) = s/(n+1)
 	--> desired property 를 만족한다!!

### Queries over sliding window
* slinding window 를 이용하면서, bit 를 세어야 한다면?
* n=1 billion 일 때에, n 에 존재하는 1인 bit 의 수를 세어야한다면? 또한 approximate 된 답이 허용된다면?
>> 2개 solution - 1. simple 하게 / 2. DGIM 써서
1. simple solution
	* Uniformity assumption
	* 두가지 카운터만 유지
		* S : stream 시작 부분에서의 1의 갯수
		* Z : stream 시작 부분에서의 0의 갯수
		* last N bit 의 1의 갯수 : N\*S/(S+Z)
	* 문제점
		1. stream 이 non-uniform 한 형태일 수 있다.
		2. distribution 이 시간에 따라 변할 수 있다.
2. DGIM
	* stream 마다 O(logN) bit 를 저장
	* error bound 를 50%이하로 유지함
	* main idea : stream 을 specific number of 1로 나눈다(1의 갯수가 일정하도록 stream 을 블럭단위로 쪼갠다) / 이 때의 block size 는 exponential 하게 증가한다!
	* 이러면 relevant timestamp 를 O(logN) bit 로 나타낼 수 있게 됨
	

