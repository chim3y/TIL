## Sampling from a data stream
### Sampling a fixed proportion
* Naive Solution
	* (user, query, time)의 stream data일때
	* hash function 로 각 query 마다 random integer 부여하고, 특정 수 아니면 다 버린다
	- stream 양이 많아지면 sample 의 양도 많아진다는 단점이 있음
	- 중복 검색어 비율 등을 구할 때에 잘못된 값을 낼 수 있음
	* 중복 검색어의 비율을 구하고싶다면?
		- user 가 평균적으로, x 개 query 를 한번, d개 query 를 2번 던진다면, 중복 검색어 비율의 참값은 d/(x+d)
		- 근데 naive 하게, 10%의 query 만 남겨뒀다면, d의 중복값이 다 들어갈 비율은 d/100(=(1/10)\*(1/10)\*d) 일 것
		- duplicate 된 애들 중에서, 18d/100(=((1/10)\*(9/10)+(1/10)\*(9/10))\*d)개는, 1번만 count 됐을 것
		- 즉, fixed proportion 으로 추출했다면, 중복 검색어의 비율은 (d/(10x+19d)) (=(d/100)/((x/10)+(d/100)+(18d/100))) 로 잘못나오게 될 것
* **Generalized solution**
	* query 가 아닌 user 를 sample 하는 방식
	* (user, query, time)에서 key 를 user 로 둔다
	* key 를 k 개 bucket 에 uniformly hashing
	* 전체 query 의 a/b 비율을 꼽는다면, b 개의 bucket 중, a 개를 sample 하는 것


### Sampling a fixed-size sample
* overview
	* 메인 메모리에 올리는게 가능한정도만 남기는 형태일 것
	* fixed size 로 sampling 하면서도, 모든 데이터가 같은 확률로 뽑힐 수 있게 할 수 있을까
		- size s 을 sampling 한다면 s가 5일때와 7일때 특정 수가 등장하는 비율이 다르게 나올거야 --> 이런 문제를 해결하려면?
	* 어떻게 하면 항상 적절하게 sampling 할 수 있을지를 고민하는 것 (전체 중에 s 개를 뽑았을때의 확률과 임의로 s개 뽑았을 떄의 확률을 같게 만드는게 목표인 것 --> sampling 을 잘 하고싶다!)
* **Reservoir Sampling**
	* 들어온 순서대로 s 개 element 저장할수 있다고 치면, 
	* n-1(n>s)개의 element 를 봤다고 할때, n 번째 element 가 도착한다면
	* --> s/n 의 확률로, n 을 pick 하고 아니면 버린다
	* 이 n 번째 element 가 pick 된다면 s개의 element 중 하나를 replace 한다.
* Reservoir Sampling 증명(inductive step)
	0. base
		- n=s 이라면 이미 desired property 만족함 --> 
	1. n번째 element 까지 했을때에, sample S 가 포함한 각 element 의 확률은 s/n
	2. n+1번째 element 가 왔을 때에
		1. n+1번째가 discard 되어 이전의 sampling 이 유지
			(1-s/(n+1))
		or
		2. n+1번째가 채택되어 뽑혀있던 s 개 중 1개가 out
			(s/(n+1))\*((s-1)/s)
		--> 1+2 = n/(n+1)
	--> 전체 element 가 n+1개일 때에 특정 tuple 이 s 에 등장할 확률 = (s/n) * (n/(n+1)) = s/(n+1)
 	--> desired property 를 만족한다!!
e.g. https://leetcode.com/problems/linked-list-random-node

### Queries over sliding window
* slinding window 를 이용하면서, bit 를 세어야 한다면?
* n=1 billion 일 때에, n 에 존재하는 1인 bit 의 수를 세어야한다면? 또한 approximate 된 답이 허용된다면?
>> 2개 solution - 1. simple 하게 / 2. DGIM 써서
1. simple solution
	* Uniformity assumption
	* 두가지 카운터만 유지
		* S : stream 시작 부분에서의 1의 갯수
		* Z : stream 시작 부분에서의 0의 갯수
		* last N bit 의 1의 갯수 : N\*S/(S+Z)
	* 문제점
		1. stream 이 non-uniform 한 형태일 수 있다.
		2. distribution 이 시간에 따라 변할 수 있다.
2. DGIM
	* stream 마다 O(logN) bit 를 저장
	* error bound 를 50%이하로 유지함
	* main idea : stream 을 specific number of 1로 나눈다(1의 갯수가 일정하도록 stream 을 블럭단위로 쪼갠다) / 이 때의 block size 는 exponential 하게 증가하게 한다(2^0, 2^1, ...) / 같은 size 를 갖는 bucket 은 1~2개로 유지함
	* DGIM bucket - 아래 두가지로 구성됨
		1. timestamp - O(logN)bit 로 나타낼 수 있음 (logN 이 곧 timestamp!)
		2. 1의 갯수 --> power of 2 일거라서, O(loglogN) 만으로도 표현 가능
	* Storage requirement
		* bucket 당 저장하는 정보 * bucket 수 = O((logN)^2)
			* bucket 당 저장 : O(logN)+O(loglogN)
			* bucket 수 : logN 개
	* bucket update
		* 2 cases 
			* current bit 가 0 : 변화 없음
			* current bit 가 1
				* 1. 현재 bit 를 위한 size 1짜리 bucket 만든다
				* 2. 이제 크기 1짜리 bucket 이 세개가 된거면 좌측으로 하나씩 merge --> 같은 size bucket 이 세개가 되지 않을때까지 merge 해나감

	* 여기에 query가 온 경우
		* most recent N bit 에 포함된 1의 수를 세고 싶은 경우
			1. 마지막 bucket 을 제외하고, 모든 bucket 의 size 를 더함(여기서의 size 는 bucket 내의 1의 갯수)
			2. 마지막 bucket size 의 절반 만큼을 더함
			* 이때 error 를 50%이하라 보장할 수 있는 이유
				* 마지막 bucket 의 size 를 2^r 이라 가정할 때
					* error 를 최대로 내는 경우라면, window까지(recent N bit) 들어가는 1인 bit 는 1개 뿐일때인데, 이 last bucket의 1의 갯수를 반 만큼 더했음.
					이때의 error 의 크기는 2^(r-1) 만큼일 것
					* 전체의 1bit 의 수는 1+2+4+...+2^(r-1)=2^r -1
					* Therefore! 전체 error 비율은 최대 50% 인 것!
	* extentison
		* error 를 더 줄이기 위해선
			* 동일한 size 의 bucket 을 1\~2개로 유지하지 않고, r-1, r size 로 조절(r>2)
			* error 률이 O(1/r)이 될 것
			* trade off - 저장해야하는 bit 의 수가 커지겠지만, 그만큼 error 비율이 줄어들 것
		* stream of positive integer 인 경우
			* last k element 의 sum 을 구하고 싶은 경우
				* e.g. avg price of last k sales
				* solution
					* 모든 integer 는 m bit이하 인 경우(최대 price를 상정해서)
						* m bits of each integer 를 separate stream 으로 treat 하자!
						* DGIM 을 각 integer 의 1을 세는 방향으로 바꿈
						* sum = sigma i=0 to (m-1) (c_i* 2^i)
							* c_i 는 estimated count for i-th bit

### Summary
* Sampling a fixed proportion of a stream
	* sample size grows as the stream grows
* Sampling a fixed-size sample
	* Reservoir sampling
* Counting the number of 1s in the last N elements
	* DGIM - exponentially increasing windows
	* Extentions
		* Number of 1s in any last k (k<N) elements
		* Sums of integers in the last N elements

