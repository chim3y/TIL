## Find similar items
### Goal
* Given
	- high dimensional data points x1, x2,...
	- distance function
* Goal : 특정 distance threshold 에서 d(xi, xj)<=S 인 모든 data point 찾기
	* naive solution 은 O(N^2)
	* --> 이걸 O(N)에 하고싶은 것

### Three essential steps for similar documents
1. shingling : document 를 set 으로 표현
	* output : set of strings of length k
2. min-hashing : 거대한 set 을 짧은 signature 로 표현(similarity 를 유지하면서)
	* output : signatures
		- signature : set을 표현하는 짧은 integer vector
3. LSH(Locality-Sensitive Hashing) : 유사 문서일 수 있는 signature pair 추출
	* output : candidate pairs
		- similarity test 가 필요한 signature pair 들

### step1. Shingling
* shingling : Document 를 order 가 유지되는 set으로
	* k-shingle(or k-gram) for document : sequence of k tokens
		- charactor 단위로 k 개씩 이어붙인 set 혹은 word 단위로 k 개씩 이어붙인 set
		- e.g. document D1=abcab --> 2-shingles: S(D1)={ab, bc, ca}
		(이때 등장횟수들을 고려할 수도 있음)

### step2. Min-hashing
* 이 set 들을 4byte로 hashing
* 즉, document 를 set of hash value(of k-shingles)로 나타냄 
	- e.g. h(D1)={1, 5, 7}
* 이 shingle 들의 similarity 를 구할 수 있음
	* 등장하는 k-shingles 의 one hot vector 로 표현한 후(very sparse vector) 이 vector 간의 jaccard similarity 를 구함
* 적절한 k 찾기
	* k = 1 이면 naive 와 같지
	* k = 2 이면 shingle 이 너무 많음
		- (거의 모든 doc 들은 아주 common 한 shingle 만 내보낼 것)
	* k 가 너무 크면 storage 문제가 발생하게 됨
		- (k=5\~10 이 일반적임)

#### Why?(motivation)
* shingle set 을 그냥 비교할때의 space complexity
	* D=N word 일 때 shingle 의 수 ==> N-k+1
	* (shingle의 수)\*(shingle 안의 word 의 수)\*(평균 word 의 길이)
	* (N-k)\*(k)\*(avg word size)
* time complexity
	* computing pairwise jaccard similarity
		* N(N-1)
* 위의 값은 너무 크다...
	* (pairwise 하게 비교시 시간이 너무 많이 걸려서 set 들을 byte 단위로 encoding)
--> 각 shingle 을 4byte number 로 hashing 함
		- --> k 개의 hash 가 나올 것

#### key idea and goal
* key idea : 각 문서들을 작은 signature h(c)로 표현함
	1. h(c)는 ram 에 적절할 정도로 작아야함
	2. sim(c1, c2)는 sim(h(c1), h(c2))와 같아야 함
* Goal : 아래 조건 만족하는 hash function 찾기
	1. sim(c1, c2)가 높으면 h(c1)=h(c2)
	2. sim(c1, c2)가 낮으면 h(c1)!=h(c2)
	- bucket 에 넣는 과정으로 생각하면, 유사한 문서들은 같은 bucket 안으로 들어가도록
	--> min hashing!(jaccard similarity 라면 이 방법이 적절)

#### min hashing
* h_pie(C)=the index of the first(in the permuted order pie) row in which column C has value 1(min_pie pie(C))
* 즉,
	1. random permutation 생성후 shingle list 도 이 기준으로 섞는다
	2. 섞인 list 중 가장 먼저 1이 나오는 row 를 return 함
* intuition :  두개의 set 이 유사할수록, min-hash의 값이 유사하게 나온다.
* signature
	* k 개의 random permutation 고른다
	* (sig(C)를 column vector 로 놓으면)
	* sig(C)[i]=i-th permutation 에 의해 생긴, column C의 1을 갖는 첫번재 row의 index
		- sig(C)[i]=min(pie_i(C))
	* k 가 100 이면, 100byte 만으로 document C를 표현하는게 가능해짐

### Step3. LSH
* similarity 가 클 만한 candidate pair 를 골라내는 용도로 사용
* For min-hash matrices
	* column 이 signature matrix M
	* LSH 했을 때에 동일 bucket 으로 들어가는 문서 쌍이 candidate pair 가 될 것
1. matrix M 을 b bands of r rows 로 만든다(각 column 이 1개 문서 / shingle 들을 b 개로 나누는 것)
2. 각 band 에 대해 k 개 bucket 의 hash table 로 해싱한다(k 는 클수록 좋음)
- candidate column pair 는 같은 bucket 으로 1개 이상의 band 에서 분류된 pair 들
- b 와 r 을 가장 유사한 pair 를 잘 잡아내고, 유사하지 않은걸 잘 걸러내도록 잘 튜닝해야함
* e.g.  c1과 c2 의 similarity 가 0.3일때, row 가 5개 인 1개의 band 가 동일한 bucket 으로 분류될 확률 = (0.3)^5 = 0.00243
	- 20개 band 중 1개 이상의 band 에서 동일하다고 나올 확률 = 1-(1-0.00243)^20
=0.0474
	- 즉, 대략 4.74% 의 pair 가 candidate pair 로 분류되게 될 것
