## 6. Statistical machine learning

### 6.1 KNN(K-nearest neighbors)

- 방법
    1. Find k records with the most similar features
    2. classification : Find out what class the majority belongs to among these similar records, and then assigns the class to the new record.
    3. prediction(regression) : Find the average of similar records and use them as predictions for new records
- distance metric
    - euclidean distance, manhattan distance
- standardization
    - 필수!
    - z-score=(x-mean)/s
- Choosing K
    - K low → overfitting / k high → over smoothing
    - 노이즈 적으면 k low / 많으면 k high (SNR) 보통 1~20
- knn 결과를 해당 record의 feature 로 사용하기도 함(Because information is narrow, it does not cause problems such as multicollinearity.)

### 6.2 Tree Model

- [https://github.com/stella-y/TIL/blob/master/MachineLearningBasic/SupervisedLearning/5_decision_tree_and_random_forest.md](https://github.com/stella-y/TIL/blob/master/MachineLearningBasic/SupervisedLearning/5_decision_tree_and_random_forest.md)
- recursive partitioning
- class purity - gini impurity(I(A)=p(1-p)) or entropy(I(A)=-plog2(p)-(1-p)log2(1-p))
- pruning
    1. minbucket / minsplit 
    2. If the impurity is not reduced to a significant degree, it doesn't divide (cp - complexity parameter)
- regression
    - Measure the impurity using the squared deviation from the mean in the sub-partition
    - evaluation - RMSE
- pros
    - 시각화가 가능함
    - Can contain nonlinear relationships between predictors
    - 일종의 규칙들의 집합

### 6.3 Bagging and Random forest

- Bagging(Bootstrap aggregating)
    - 만들 모델 갯수 M, 사용할 레코드 n →n개 boostrap sampling → 모델 학습 → M개를 평균한 게 최종 모델
- Random forest
    - boostrap sampling → p개 변수만 random sampling → tree 학습 → M 개를 평균한 게 최종 모델
    - cons : black box 모델이 돼버림
- 변수 중요도
    - type1 - 변수 값을 랜덤하게 섞었다면 모델의 정확도가 감소하는 정도를 측정
    - type2 - Measure the average decrease in the impurity score at all nodes where segmentation occurs based on a specific variable.(요게 더 일반적)
- hyperparameter
    - nodesize, maxnodes - get it from k-fold validation

### 6.4 Boosting

- A common technique for generating a series of models by weighting records with large residuals in each successive round.
- AdaBoost - with residual / Gradient Boosting - cost function minimize / stochastic gradient boosting - resample row and columns on each round
- xgboost - open source
    - parameter - subsample(샘플링할 입력데이터의 비율), eta(alpha에 적용되는 축소 비율)
- regularization
    - To prevent overfitting...
    - Lasso (L1), Ridge (L2)
