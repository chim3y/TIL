## 세가지 embedding 방법론
-- word2vec / glove / fasttext
-- 단어 동시 등장 정보(word's of co-occurrence) 보존 하는 점에서 빈도수 기반의 기존 방법론들과 본질적으로 다르지는 않음
https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/07/06/fasttext/

### word2vec
* 단어를 벡터로
* CBOW / Skip-Gram
	* CBOW - 주변에 있는 단어들로 중심에 있는 단어를 맞추는 방식
		```
		나는 ____에 간다.
		```
		주변 단어를 가지고 중심에 있는 단어를 맞춤으로써 단어 벡터들을 만들어내는 방법
	* skip-gram - 중심에 있는 단어로 주변 단어를 예측하는 방식
		```
		__ 외나무다리 __
		```
		여기서 앞 뒤에 어떤 단어가 올지 예측
		학습시킬 말뭉치에 ‘외나무다리’ 뒤에 ‘-에서’, ‘만난다’는 표현이 나왔다고 치자. 그러면 Word2Vec은 ‘외나무다리’가 ‘-에서’, ‘만난다’와 어떤 연관이 있다고 보고 이를 감안해서 단어를 벡터로 만들게 됨
* 의미가 같지는 않아도 자주 쓰이는 연어(collocation)을 찾아낼 수 있게 됨 (ex 외나무다리 & 원수)
* 임베딩된 두 단어벡터의 내적 = 코사인 유사도
* 목적함수
	* skip gram : 중심 단어가 주어졌을 때 주변 단어가 등장할 조건부확률이 최대가 되도록 학습
	* ![word2vec](images/1_1.png "word2vec")
	* 분자 증가시키기
		* 분자를 증가시킨다 -->  exp의 지수를 크게 한다
		* exp의 지수 = 두 벡터의 내적값
		* 이 값이 커진다는 건 벡터들 사이의 θ를 줄인다(즉 유사도를 높인다)는 말
		* **다시 말해 중심단어(c)와 주변단어(o)를 벡터공간에 뿌릴 때 인근에 위치시킨다**
	* 분모 줄이기
		* 분모는 중심단어(c)와 학습 말뭉치 내 모든 단어를 각각 내적한 것의 총합
		* 분모를 줄이려면 주변에 등장하지 않은 단어와 중심단어와의 내적값은 작아져야 
		* **중심단어 주변에 등장하지 않은 단어에 해당하는 벡터와 중심단어 벡터 사이의 θ를 키운다(코사인 유사도를 줄인다)는 의미**
* Word2Vec은 기존 count 기반 방법론처럼 자주 같이 등장하는 단어들의 정보(Co-occurrence)를 보존
* 한계점
	* 단어의 형태학적 특성을 반영하지 못함
		- 단어들을 개별적으로 embedding 하기 때문에 형태적으로 비슷한 teach, teacher teachers 등의 단어에 대한 vector 가 유사하게 구성되지 않는다.
	* 희소한 단어를 Embedding하기 어려움
		- Word2Vec등과 같은 기존의 방법들은 Distribution hypothesis를 기반으로 학습하는 것이기 때문에, 출현횟수가 많은 단어에 대해서는 잘 Embedding이 되지만, 출현횟수가 적은 단어에 대해서는 제대로 Embedding이 되지 않는다.
		- (Machine learning에서, Sample이 적은 단어에 대해서는 Underfitting이 되는 것처럼)
	* Out-of-Vocabulary(OOV)를 처리할 수 없는 단점 
		- Word2Vec은 단어단위로 어휘집(Vocabulary)를 구성하기 때문에, 어휘집에 없는 새로운 단어가 등장하면 데이터 전체를 다시 학습시켜야 함
https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/11/embedding/
https://inspiringpeople.github.io/data%20analysis/word_embedding/


### Glove
* 보존하려는 정보 --> 단어 동시 등장 여부
* 임베딩된 단어 벡터끼리의 내적 = 동시 등장확률의 로그값과 같음
* 단어 동시 등장 여부 보존함
https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/04/09/glove/

### Fasttext
* 원래 단어를 부분단어(subword)의 벡터들로 표현
	* 단어를 Bag-of-Characters로 보고, 개별 단어가 아닌 n-gram의 Charaters를 Embedding
* 최종의 각 단어들은 embedding 된 각 n-gram 의 합으로 표현됨
* --> 형태학적인 특성 반영이 가능해진다
* 이거 빼면 word2vec과 거의 유사함
* 노이즈 많은 말뭉치에 강점이 있음
* 한국어 pretrained fasttext
	* 한국어의 경우 wiki.ko.bin / wiki.ko.vec 파일
	* https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md
	
https://research.fb.com/fasttext/
https://inspiringpeople.github.io/data%20analysis/word_embedding/



